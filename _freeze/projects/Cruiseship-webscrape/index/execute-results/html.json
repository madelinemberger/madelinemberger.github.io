{
  "hash": "c39164c559fd7661f14db740cc9e06cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Wikipedia webscrape to generate a dataset of cruiseships and their passenger capacities\"\ndescription: \"Using the rvest R package to help estimate cruiseship impact on marine habitats\"\nauthor: \"Madeline Berger\"\ndate: 2021-06-11\ncategories: [Webscraping, R, Cruiseships, Pollution]\nimage: images/polina-rytova-unsplash.jpg\nstatus: completed\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    code-fold: show\n    page-layout: full\n    theme: cosmo\n---\n\n## Project Overview\n\nFor my paper quantifying nutrient pollution in the Mesoamerican Reef, I wanted to explore ways to include pollution generated from cruise ships in our analysis. The large-scale cruise industry has become somewhat notorious for poor environmental practices (including dredging reefs to accommodate increasingly enormous ships, pollution, whale strikes...), so its perhaps not surprising that there is very little data publicly available. I did [find a page on Wikipedia](https://en.wikipedia.org/wiki/List_of_cruise_ships) listing all known ships both in service and ones that have ceased to operate.\n\nGiven the long list, I decided to try out the `rvest` package to see if I could just scrape the information I wanted from the webpage. The rest of this post walks through my process and code.\n\n\\*thumbnail by photo by [Polina Rytova](https://unsplash.com/@polina_art?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash) on [Unsplash](https://unsplash.com/photos/aerial-photography-of-white-and-brown-cruise-ship-on-water-1AUe0hwdC3o?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)\n\n\n\nFirst, I need to examine the webpage to understand what elements I want. In this case, I want all the cruise ship names. Then, I want to go onto each cruises own page and grab the number that is under the \"capacity\" row in the right hand summary table.\n\nTo scrape stuff from the web, you need to look \"under the hood\" at the site code, and ID the xpath of the information you are interested in targeting.\n\n**Cruise ship names from full list**\n\nurl = '[https://en.wikipedia.org/wiki/List_of_cruise_ships'](https://en.wikipedia.org/wiki/List_of_cruise_ships')\n\n#### Xpaths for the first 3 tables (A - C) :\n\n//[@id*=\"mw-content-text\"*]*/div\\[1\\]/table\\[2\\] //*[@id=\"mw-content-text\"]/div\\[1\\]/table\\[3\\] //\\*[@id=\"mw-content-text\"]/div\\[1\\]/table\\[4\\]\n\n#### Xpaths for the names of the cruiseships:\n\n//\\*[@id=\"mw-content-text\"]/div\\[1\\]/table\\[2\\]/tbody/tr\\[42\\]/th/i/a\n\n#### Xpaths for the capacity element on a ship page:\n\n//\\*[@id=\"mw-content-text\"]/div\\[1\\]/table\\[1\\]/tbody/tr\\[30\\]/td\\[1\\]\n\n# Getting Ship Page URLs\n\nFirst get a list of URLs to each ships' own wikipedia page\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Step 1: Define URL ####\n\nurl = 'https://en.wikipedia.org/wiki/List_of_cruise_ships'\nurl2 = read_html('https://en.wikipedia.org/wiki/List_of_cruise_ships')\n\n### Step 2: Pull using the Xpath for the name element in the table OR using the html type ###\n\n# this works to get ship names\nsample1 = url %>%\n  read_html() %>%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[2]') %>%\n  html_table(fill = TRUE) %>% \n  pluck(1) %>% #this kind of works\n  as.data.frame()\n\nsample1 <- sample1[[1]] #this extracts the dataframe\n\n\n# this works to get ship links - but grabs ALL links on the page\n\nlinks <- url2 %>% \n  html_nodes(\"a\") %>% \n  html_attr(\"href\") %>% \n  tibble::enframe() %>% \n  as.data.frame()\n\nhead(links)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  name                       value\n1    1                #bodyContent\n2    2             /wiki/Main_Page\n3    3    /wiki/Wikipedia:Contents\n4    4 /wiki/Portal:Current_events\n5    5        /wiki/Special:Random\n6    6       /wiki/Wikipedia:About\n```\n\n\n:::\n:::\n\n\nClean up `links` so that we just have a cruise ship name and its info. Looking at it, we can see there is a bunch of stuff we don't want. We really only want the links that start with `/wiki/`. And we also don't want any of the first 56 rows or the last batch of rows as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nship_links <- links %>% \n  rename(idnum = name, link = value) %>%\n  mutate(\n    idnum = as.numeric(idnum)\n  ) %>% \n  filter(idnum > 56) %>%\n  filter(str_detect(link,pattern = \"/wiki/\")) %>% #all this is stuff I saw while visually inspecting the dataset\n  filter(!str_detect(link, pattern = \"_Cruises\") & \n           !str_detect(link, pattern = \"_Line\") & \n           !str_detect(link, pattern = \"Tonnage\") &\n           !str_detect(link, pattern = \"Ocean_liner\") & \n           !str_detect(link, pattern = \"Cruise_%26_Maritime_Voyages\") & \n           !str_detect(link, pattern = \"_International\") &\n           !str_detect(link, pattern = \"-class\")&\n           !str_detect(link, pattern = \":\") &\n           !str_detect(link, pattern = \"%\")\n  ) %>% \n  filter(idnum < 1105) %>% #I found the last ship and got rid of all the links below it\n  mutate(\n    full_link = paste0(\"https://en.wikipedia.org\",link) #create full link\n  ) \n\n\nhead(ship_links)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  idnum                          link\n1    74             /wiki/Cruise_ship\n2    76    /wiki/List_of_ocean_liners\n3    81        /wiki/MS_Achille_Lauro\n4    83                /wiki/Nedlloyd\n5    84 /wiki/Achille_Lauro_hijacking\n6    86              /wiki/MS_Adriana\n                                              full_link\n1             https://en.wikipedia.org/wiki/Cruise_ship\n2    https://en.wikipedia.org/wiki/List_of_ocean_liners\n3        https://en.wikipedia.org/wiki/MS_Achille_Lauro\n4                https://en.wikipedia.org/wiki/Nedlloyd\n5 https://en.wikipedia.org/wiki/Achille_Lauro_hijacking\n6              https://en.wikipedia.org/wiki/MS_Adriana\n```\n\n\n:::\n:::\n\n\nNow, we want to extract the name of the ship, which will be the last phrase of the link. Two options:\n\n1.  Use str to extract it from the link\n2.  Pull all the cruise names using `rvest` and then do some kind of str_detect to match it to capacity?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nship_links_full <- ship_links %>% \n  mutate(\n    tmp_chunks = str_split(link, fixed(\"/\")) #split up the link\n  ) %>% \n  mutate(\n    name = map_chr(tmp_chunks, 3) #extract the third element, which is the name\n  ) %>% \n  mutate(\n    name = str_to_upper(str_replace_all(name,\"_\",\" \"))\n  ) %>% \n  dplyr::select(-tmp_chunks) %>% \n  rename(href = link)\n\n# save \n\nwrite_csv(ship_links_full, \"outputs/ship_links_clean.csv\")\n```\n:::\n\n\nNow lets see if we can just filter out the ships we know are in the MAR region in 2019. I converted the names to uppercase to match the Global Fishing Watch data, but due to some being identified in either dataset with just their abbreviations it may not be perfect. Let's give it a shot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nship_links_clean <- read_csv(file.path(dir_interim,\"/cruiseships/ship_links_clean.csv\"))\n\ngfw_joined <- read_csv(file.path(dir_interim,\"cruiseships/cruiseships_2019.csv\"))\n\nnames <- unique(ship_links_full$name)\n\n#ships in MAR region - 72\nMAR_names <- unique(gfw_joined$shipname)\n\nship_links_MAR <- ship_links_clean %>% \n  filter(name %in% MAR_names)\n```\n:::\n\n\nDarn - that only got 45 of them. Let's see if at least those 45 work to scrape the data. The rest I can fill in by hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmissing_df <- data.frame(\n  missing_names = setdiff(gfw_joined$shipname, ship_links_MAR$name)\n) %>% \n  mutate(\n    name = as.character(missing_names)\n  ) %>% \n  mutate(\n    full_link = case_when(\n      name %in% \"FRAM\" ~ \"https://en.wikipedia.org/wiki/MS_Fram\",\n      name %in% \"SILVER WIND\" ~ \"https://en.wikipedia.org/wiki/Silver_Wind\",\n      name %in% \"NAVIGATOR OF THE SEA\" ~ \"https://en.wikipedia.org/wiki/Navigator_of_the_Seas\",\n      name %in% \"OOSTERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Oosterdam\",\n      name %in% \"ZUIDERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Zuiderdam\",\n      name %in% \"ISLAND PRINCESS\" ~ \"https://en.wikipedia.org/wiki/MS_Island_Princess_(2002)\",\n      name %in% \"ORIANA\" ~ \"https://en.wikipedia.org/wiki/MV_Piano_Land\",\n      name %in% \"REGATTA\" ~ \"https://en.wikipedia.org/wiki/MS_Regatta\",\n      name %in% \"AURORA\" ~ \"https://en.wikipedia.org/wiki/MV_Aurora_(2000)\",\n      name %in% \"CELEBRITYSILHOUETTE\" ~ \"https://en.wikipedia.org/wiki/Celebrity_Silhouette\",\n      name %in% \"NIEUW AMSTERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Marella_Spirit\",\n      name %in% \"EURODAM\" ~ \"https://en.wikipedia.org/wiki/MS_Eurodam\",\n      name %in% \"CARNIVAL MIRACLE TB1\" ~ \"https://en.wikipedia.org/wiki/Carnival_Miracle\",\n      name %in% \"VIKING SUN\" ~ \"https://en.wikipedia.org/wiki/MS_Amera\",\n      name %in% \"CARIBBEANPRINCE TB22\" ~ \"https://en.wikipedia.org/wiki/MS_Eurodam\",\n      name %in% \"NCL GETAWAY T6\" ~ \"https://en.wikipedia.org/wiki/Norwegian_Getaway\",\n      name %in% \"CARNIVAL MIRACLE TB5\" ~ \"https://en.wikipedia.org/wiki/Carnival_Miracle\",\n      name %in% \"M/S ROTTERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Borealis\",\n      name %in% \"VEENDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Aegean_Majesty\",\n      name %in% \"INDEPENDENCE OF SEAS\" ~ \"https://en.wikipedia.org/wiki/Independence_of_the_Seas\",\n      name %in% \"ADVENTURE OF THE SEA\" ~ \"https://en.wikipedia.org/wiki/Adventure_of_the_Seas\",\n      name %in% \"BRILLIANCE OFTHESEAS\" ~ \"https://en.wikipedia.org/wiki/Brilliance_of_the_Seas\",\n      name %in% \"ENCHANTMENT OTS\" ~ \"https://en.wikipedia.org/wiki/MS_Aegean_Majesty\",\n      TRUE ~ \"NA\"\n    )\n  ) %>% \n  filter(!full_link == \"NA\") %>% \n  select(-missing_names, full_link, name)\n\n# join with ship links MAR\n\nship_links_MAR_all <- ship_links_MAR %>% \n  dplyr::select(full_link,name) %>% \n  rbind(missing_df)\n\nwrite_csv(ship_links_MAR_all, file.path(dir_interim,\"/cruiseships/ship_links_all.csv\"))\n```\n:::\n\n\nList of ships we could not find info for:\n\n-   M/V Hamburg\n-   RCGS Resolute\n-   Seven Seas Voyager 7 (we already have the seven seas voyager so not sure what this ship is)\n-   CaribbeanPrince TB22 (we already have Caribbean Princess?)\n-   Carnival Triumph (could not find)\n\nOther notes: Carnival Miracle TB5 is the same as Carnival Miracle, based on IMO number MS FRAM taken out below before running loop because it doesn't have a crew \\#\n\n# Scrape passenger number from each ships' page\n\nTry using a loop - not as nice but the above is simply not working.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in links csv\nship_links_MAR_all <- read_csv(file.path(dir_interim,\"/cruiseships/ship_links_all.csv\")) %>% \n  filter(!name == \"FRAM\")\n\nurls <- ship_links_MAR_all$full_link\n\n#urls_test <- urls[1:55]\n\n#create empty list \n\ncapacities_list <- list()\n\n#fill list by looping over each link in vector\n\nfor (i in seq_along(urls)) {\n  \n   #x = urls[[46]]\n  \n    x = urls[i]\n  \n   table <-  read_html(x) %>% \n    rvest::html_nodes(\"table.infobox\") %>% \n    rvest::html_table(header=F, fill = T) \n  \n   table2 <- table[[1]] %>% \n    clean_names() %>% \n    rename(\n      name = 1, \n      value = 2\n    ) %>% \n     filter(name != \"\") %>%\n     pivot_wider(names_from = name,\n                 values_from = value) \n   \n   table3 <- table2 %>% \n       dplyr::select(`Capacity:`, `Crew:`) %>%\n       mutate(\n       full_link = x,\n       capacity = as.character(`Capacity:`),\n       crew = as.character(`Crew:`)\n       ) %>% \n      dplyr::select(-`Capacity:`, -`Crew:`)\n   \n   print(i)\n\n   capacities_list[[i]] <- table3\n   \n}\n\n#unlist\ncapacities_MAR = do.call(rbind, capacities_list)\n```\n:::\n\n\nLast step is to clean up the ship numbers using str - I'll just split it based on the space, and then grab the first element. Also need to add back in the MS FRAM since we took it out of the loop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#best way to get numbers out\ncapacities_MAR_clean <- capacities_MAR %>%\n   mutate(\n    capacity_n = as.numeric(parse_number(capacity)),\n    crew_n = as.numeric(parse_number(crew))\n  ) %>% \n  mutate(\n    total_ship_pop = crew_n + capacity_n\n  ) %>% \n  dplyr::select(-crew, -capacity)\n\n#rejoin with shipnames and links \n\n#FRAM df\n\nfram <- data.frame(\n  shipname = \"FRAM\",\n  capacity_n = 400,\n  crew_n = 0,\n  total_ship_pop = 400\n)\n\ncruise_capacities_final <- full_join(capacities_MAR_clean, ship_links_MAR_all, by = \"full_link\") %>% \n  dplyr::select(shipname = name,capacity_n,crew_n,total_ship_pop) %>% \n  rbind(fram)\n\n\nwrite_csv(cruise_capacities_final, file.path(dir_interim,\"/cruiseships/cruise_capacities_final.csv\"))\n```\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}