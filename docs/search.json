[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "As spatial planning, and especially marine spatial planning, gains more momentum as a management tool for healthy environments, organizations have been creating helpful interactive map tools to view and assess data. Some of these sources also provide data, and are included in my “user friendly / wrangle level” graphic."
  },
  {
    "objectID": "resources.html#map-tools",
    "href": "resources.html#map-tools",
    "title": "Resources",
    "section": "",
    "text": "As spatial planning, and especially marine spatial planning, gains more momentum as a management tool for healthy environments, organizations have been creating helpful interactive map tools to view and assess data. Some of these sources also provide data, and are included in my “user friendly / wrangle level” graphic."
  },
  {
    "objectID": "resources.html#ocean-data-sources",
    "href": "resources.html#ocean-data-sources",
    "title": "Resources",
    "section": "Ocean Data Sources",
    "text": "Ocean Data Sources\nThere is tons of data to download out there, but even after 3+ years working with marine data I realized that I still started most of my data hunts with a generic Google search. This section to start recording different sources I’ve found useful, both for my own and hopefully for others’ benefit."
  },
  {
    "objectID": "resources.html#open-source-science",
    "href": "resources.html#open-source-science",
    "title": "Resources",
    "section": "Open Source Science",
    "text": "Open Source Science\nI was incredibly fortunate to launch my career in conservation at the National Center for Ecological Analysis and Synthesis, where open and reproducible science is at the forefront of every project. These are some resources I found most helpful when thinking about how to structure projects, and code, in a way that is clear and easy to share with others."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madeline Berger",
    "section": "",
    "text": "I am a data scientist and spatial ecologist, currently working to support marine conservation and management in Hawaiʻi. I am passionate about leveraging open source tools to support good science and participatory decision making."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Madeline Berger",
    "section": "Education",
    "text": "Education\nPhD in Marine Biology in Progress (expected 2027) | University of Hawaiʻi at Mānoa | Honolulu, HI\nMESM in Conservation Planning & Data Science (2020) | University of California, Santa Barbara | Santa Barbara, CA\nBA in Economics (2016) | University of California, Los Angeles | Los Angeles, CA"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Madeline Berger",
    "section": "Experience",
    "text": "Experience\nFreelance Data Scientist, 2024 - Present\nSpatial Analysis and Data Specialist, 2022 - Present | Hawaii Monitoring and Reporting Collaborative (HIMARC)\nSpatial Analyst and Modeler, September 2020 - October 2022 | National Center for Ecological Analysis & Synthesis & SeaSketch\nOcean Health Index Fellow, March 2020 - September 2020 | National Center for Ecological Analysis & Synthesis"
  },
  {
    "objectID": "projects/Mapbox-maps/index.html",
    "href": "projects/Mapbox-maps/index.html",
    "title": "Dynamic cartography using Mapbox",
    "section": "",
    "text": "Working as a Marine Spatial Planning analyst with the McClintock lab had the added benefit that I got to know and learn how to use the latest advances in open source geospatial and mapping tools, as SeaSketch shifted towards its own completely open source design.\nOne of those tools was Mapbox, and I found myself really enjoying the streamlined and transparent process of adding and then styling data using this tool. I’ve since used Mapbox to create maps for independent projects I’ve worked on free. In this post I’ll discuss the basics of using Mapbox (specifically from the POV of a non-commercial user), and some helpful tips on how to transition data from Esri-optimized formats (shp, tif) to geojson."
  },
  {
    "objectID": "projects/Mapbox-maps/index.html#what-is-mapbox",
    "href": "projects/Mapbox-maps/index.html#what-is-mapbox",
    "title": "Dynamic cartography using Mapbox",
    "section": "What is Mapbox?",
    "text": "What is Mapbox?\n\nGetting Set Up\n\n\nCartography Syntax"
  },
  {
    "objectID": "projects/Mapbox-maps/index.html#helper-scripts-for-data-management",
    "href": "projects/Mapbox-maps/index.html#helper-scripts-for-data-management",
    "title": "Dynamic cartography using Mapbox",
    "section": "Helper Scripts for Data Management",
    "text": "Helper Scripts for Data Management\n\nGeojson versus shape - why it matters\n\n\nGeojson.io"
  },
  {
    "objectID": "projects/Cruiseship-webscrape/index.html",
    "href": "projects/Cruiseship-webscrape/index.html",
    "title": "Webscraping for Cruiseship Information",
    "section": "",
    "text": "For my paper quantifying nutrient pollution in the Mesoamerican Reef, I wanted to explore ways to include pollution generated from cruise ships in our analysis. The large-scale cruise industry is notorious for poor environmental practices (including dredging reefs to accommodate increasingly enormous ships, pollution, whale strikes…), so it’s perhaps not surprising that there is very little data publicly available. I did find a page on Wikipedia listing all known cruise ships (both in service and retired) with details about the ship capacity and build, which seemed to be the best source available.\nGiven the long list, I decided to try out the rvest package to see if I could just scrape the information I wanted from the webpage. The rest of this post walks through my process and code. I already had a list of cruiseships that were present in the Mesoamerican Reef region from Global Fishing Watch, but I needed additional information about their passenger numbers to estimate the amount of waste water each of these ships might be generating.\n*thumbnail by photo by Polina Rytova on Unsplash"
  },
  {
    "objectID": "projects/Cruiseship-webscrape/index.html#project-overview",
    "href": "projects/Cruiseship-webscrape/index.html#project-overview",
    "title": "Webscraping for Cruiseship Information",
    "section": "",
    "text": "For my paper quantifying nutrient pollution in the Mesoamerican Reef, I wanted to explore ways to include pollution generated from cruise ships in our analysis. The large-scale cruise industry is notorious for poor environmental practices (including dredging reefs to accommodate increasingly enormous ships, pollution, whale strikes…), so it’s perhaps not surprising that there is very little data publicly available. I did find a page on Wikipedia listing all known cruise ships (both in service and retired) with details about the ship capacity and build, which seemed to be the best source available.\nGiven the long list, I decided to try out the rvest package to see if I could just scrape the information I wanted from the webpage. The rest of this post walks through my process and code. I already had a list of cruiseships that were present in the Mesoamerican Reef region from Global Fishing Watch, but I needed additional information about their passenger numbers to estimate the amount of waste water each of these ships might be generating.\n*thumbnail by photo by Polina Rytova on Unsplash"
  },
  {
    "objectID": "projects/Cruiseship-webscrape/index.html#tools-and-libraries-used",
    "href": "projects/Cruiseship-webscrape/index.html#tools-and-libraries-used",
    "title": "Webscraping for Cruiseship Information",
    "section": "Tools and libraries used",
    "text": "Tools and libraries used\n\nWrangling: tidyverse, janitor , stringr\nScraping: rvest,\nLooping: purr, foreach\nTables: gt"
  },
  {
    "objectID": "projects/Cruiseship-webscrape/index.html#code",
    "href": "projects/Cruiseship-webscrape/index.html#code",
    "title": "Webscraping for Cruiseship Information",
    "section": "Code",
    "text": "Code\nFirst, I need to examine the webpage to understand what elements I want. In this case, I want all the cruise ship names. Then, I want to go onto each cruises own page and grab the number that is under the “capacity” row in the right hand summary table.\nTo scrape stuff from the web, you need to look “under the hood” at the site code, and ID the xpath of the information you are interested in targeting.\nCruise ship names from full list\nurl = ’https://en.wikipedia.org/wiki/List_of_cruise_ships\n\nXpaths for the first 3 tables (A - C) :\n//[@id*=\"mw-content-text\"*]/div[1]/table[2] //[@id=\"mw-content-text\"]/div[1]/table[3] //*[@id=\"mw-content-text\"]/div[1]/table[4]\n\n\nXpaths for the names of the cruiseships:\n//*[@id=\"mw-content-text\"]/div[1]/table[2]/tbody/tr[42]/th/i/a\n\n\nXpaths for the capacity element on a ship page:\n//*[@id=\"mw-content-text\"]/div[1]/table[1]/tbody/tr[30]/td[1]\n\n\nGetting Ship Page URLs\nFirst get a list of URLs to each ships’ own wikipedia page\n\n\nCode\n### Step 1: Define URL ####\n\nurl = 'https://en.wikipedia.org/wiki/List_of_cruise_ships'\nurl2 = read_html('https://en.wikipedia.org/wiki/List_of_cruise_ships')\n\n### Step 2: Pull using the Xpath for the name element in the table OR using the html type ###\n\n# this works to get ship names\nsample1 = url %&gt;%\n  read_html() %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[2]') %&gt;%\n  html_table(fill = TRUE) %&gt;% \n  pluck(1) %&gt;% #this kind of works\n  as.data.frame()\n\nsample1 &lt;- sample1[[1]] #this extracts the dataframe\n\n\n# this works to get ship links - but grabs ALL links on the page\n\nlinks &lt;- url2 %&gt;% \n  html_nodes(\"a\") %&gt;% \n  html_attr(\"href\") %&gt;% \n  tibble::enframe() %&gt;% \n  as.data.frame()\n\ngt(head(links))\n\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\n1\n#bodyContent\n\n\n2\n/wiki/Main_Page\n\n\n3\n/wiki/Wikipedia:Contents\n\n\n4\n/wiki/Portal:Current_events\n\n\n5\n/wiki/Special:Random\n\n\n6\n/wiki/Wikipedia:About\n\n\n\n\n\n\n\nClean up links so that we just have a cruise ship name and its info. Looking at it, we can see there is a bunch of stuff we don’t want. We really only want the links that start with /wiki/. And we also don’t want any of the first 56 rows or the last batch of rows as well.\n\n\nCode\nship_links &lt;- links %&gt;% \n  rename(idnum = name, link = value) %&gt;%\n  mutate(\n    idnum = as.numeric(idnum)\n  ) %&gt;% \n  filter(idnum &gt; 56) %&gt;%\n  filter(str_detect(link,pattern = \"/wiki/\")) %&gt;% #all this is stuff I saw while visually inspecting the dataset\n  filter(!str_detect(link, pattern = \"_Cruises\") & \n           !str_detect(link, pattern = \"_Line\") & \n           !str_detect(link, pattern = \"Tonnage\") &\n           !str_detect(link, pattern = \"Ocean_liner\") & \n           !str_detect(link, pattern = \"Cruise_%26_Maritime_Voyages\") & \n           !str_detect(link, pattern = \"_International\") &\n           !str_detect(link, pattern = \"-class\")&\n           !str_detect(link, pattern = \":\") &\n           !str_detect(link, pattern = \"%\")\n  ) %&gt;% \n  filter(idnum &lt; 1105) %&gt;% #I found the last ship and got rid of all the links below it\n  mutate(\n    full_link = paste0(\"https://en.wikipedia.org\",link) #create full link\n  ) \n\n\ngt(head(ship_links))\n\n\n\n\n\n\n\n\nidnum\nlink\nfull_link\n\n\n\n\n74\n/wiki/Cruise_ship\nhttps://en.wikipedia.org/wiki/Cruise_ship\n\n\n76\n/wiki/List_of_ocean_liners\nhttps://en.wikipedia.org/wiki/List_of_ocean_liners\n\n\n81\n/wiki/MS_Achille_Lauro\nhttps://en.wikipedia.org/wiki/MS_Achille_Lauro\n\n\n83\n/wiki/Nedlloyd\nhttps://en.wikipedia.org/wiki/Nedlloyd\n\n\n84\n/wiki/Achille_Lauro_hijacking\nhttps://en.wikipedia.org/wiki/Achille_Lauro_hijacking\n\n\n86\n/wiki/MS_Adriana\nhttps://en.wikipedia.org/wiki/MS_Adriana\n\n\n\n\n\n\n\nNow, we want to extract the name of the ship, which will be the last phrase of the link. Two options:\n\nUse str to extract it from the link\nPull all the cruise names using rvest and then do some kind of str_detect to match it to capacity?\n\n\n\nCode\nship_links_full &lt;- ship_links %&gt;% \n  mutate(\n    tmp_chunks = str_split(link, fixed(\"/\")) #split up the link\n  ) %&gt;% \n  mutate(\n    name = map_chr(tmp_chunks, 3) #extract the third element, which is the name\n  ) %&gt;% \n  mutate(\n    name = str_to_upper(str_replace_all(name,\"_\",\" \"))\n  ) %&gt;% \n  dplyr::select(-tmp_chunks) %&gt;% \n  rename(href = link)\n\n# save \n\n#write_csv(ship_links_full, \"outputs/ship_links_clean.csv\")\n\n\nNow lets see if we can just filter out the ships we know are in the MAR region in 2019. I converted the names to uppercase to match the Global Fishing Watch data, but due to some being identified in either dataset with just their abbreviations it may not be perfect. Let’s give it a shot.\n\n\nCode\nship_links_clean &lt;- read_csv(\"outputs/ship_links_clean.csv\")\n\ngfw_joined &lt;- read_csv(\"data/cruiseships_2019.csv\") # list of location and length of stay per pixle of cruise ships, filtered for MAR region, 2019. Acquired from Global Fishing Watch\n\nnames &lt;- unique(ship_links_clean$name)\n\n#ships in MAR region - 72\n\nMAR_names &lt;- unique(gfw_joined$shipname) # 72 unique named ships\n\n# filter the links using the 72 cruise ship names that we know were present in the MAR region \nship_links_MAR &lt;- ship_links_clean %&gt;% \n  filter(name %in% MAR_names)\n\n\nUsing a simple filter matching names from the links to the spatial dataset only got 38 of them. Let’s see if at least those 38 work to scrape the data. The rest I can fill in by hand:\n\n\nCode\nmissing_df &lt;- data.frame(\n  missing_names = setdiff(gfw_joined$shipname, ship_links_MAR$name)\n) %&gt;% \n  mutate(\n    name = as.character(missing_names)\n  ) %&gt;% \n  mutate(\n    full_link = case_when(\n      name %in% \"FRAM\" ~ \"https://en.wikipedia.org/wiki/MS_Fram\",\n      name %in% \"SILVER WIND\" ~ \"https://en.wikipedia.org/wiki/Silver_Wind\",\n      name %in% \"NAVIGATOR OF THE SEA\" ~ \"https://en.wikipedia.org/wiki/Navigator_of_the_Seas\",\n      name %in% \"OOSTERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Oosterdam\",\n      name %in% \"ZUIDERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Zuiderdam\",\n      name %in% \"ISLAND PRINCESS\" ~ \"https://en.wikipedia.org/wiki/MS_Island_Princess_(2002)\",\n      name %in% \"ORIANA\" ~ \"https://en.wikipedia.org/wiki/MV_Piano_Land\",\n      name %in% \"REGATTA\" ~ \"https://en.wikipedia.org/wiki/MS_Regatta\",\n      name %in% \"AURORA\" ~ \"https://en.wikipedia.org/wiki/MV_Aurora_(2000)\",\n      name %in% \"CELEBRITYSILHOUETTE\" ~ \"https://en.wikipedia.org/wiki/Celebrity_Silhouette\",\n      name %in% \"NIEUW AMSTERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Marella_Spirit\",\n      name %in% \"EURODAM\" ~ \"https://en.wikipedia.org/wiki/MS_Eurodam\",\n      name %in% \"CARNIVAL MIRACLE TB1\" ~ \"https://en.wikipedia.org/wiki/Carnival_Miracle\",\n      name %in% \"VIKING SUN\" ~ \"https://en.wikipedia.org/wiki/MS_Amera\",\n      name %in% \"CARIBBEANPRINCE TB22\" ~ \"https://en.wikipedia.org/wiki/MS_Eurodam\",\n      name %in% \"NCL GETAWAY T6\" ~ \"https://en.wikipedia.org/wiki/Norwegian_Getaway\",\n      name %in% \"CARNIVAL MIRACLE TB5\" ~ \"https://en.wikipedia.org/wiki/Carnival_Miracle\",\n      name %in% \"M/S ROTTERDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Borealis\",\n      name %in% \"VEENDAM\" ~ \"https://en.wikipedia.org/wiki/MS_Aegean_Majesty\",\n      name %in% \"INDEPENDENCE OF SEAS\" ~ \"https://en.wikipedia.org/wiki/Independence_of_the_Seas\",\n      name %in% \"ADVENTURE OF THE SEA\" ~ \"https://en.wikipedia.org/wiki/Adventure_of_the_Seas\",\n      name %in% \"BRILLIANCE OFTHESEAS\" ~ \"https://en.wikipedia.org/wiki/Brilliance_of_the_Seas\",\n      name %in% \"ENCHANTMENT OTS\" ~ \"https://en.wikipedia.org/wiki/MS_Aegean_Majesty\",\n      TRUE ~ \"NA\"\n    )\n  ) %&gt;% \n  filter(!full_link == \"NA\") %&gt;% \n  select(-missing_names, full_link, name)\n\n# join with ship links MAR\n\nship_links_MAR_all &lt;- ship_links_MAR %&gt;% \n  dplyr::select(full_link,name) %&gt;% \n  rbind(missing_df)\n\nwrite_csv(ship_links_MAR_all, \"outputs/ship_links_all.csv\")\n\nmissing_names = setdiff(gfw_joined$shipname, ship_links_MAR_all$name)\n\n\nList of ships we could not match to links:\n\nM/V Hamburg\nCosta Luminosa\nSerenade of the Seas\nSeven Seas Mariner\nSea Cloud II\nRCGS Resolute\nSeven Seas Voyager 7\nSeabourn Quest\nVision of the seas\nCarnival Triumph (could not find)\nCarnival Victory\nSeven Seas Voyager\n\nOther notes:\n\nCarnival Miracle TB5 is the same as Carnival Miracle, based on IMO number\nMS FRAM taken out below before running loop because it doesn’t have a crew #\n\n\n\nScrape passenger number from each ships’ page\n\n\nCode\n#read in links csv if starting from here\nship_links_MAR_all &lt;- read_csv(\"outputs/ship_links_all.csv\") %&gt;% \n  filter(!name == \"FRAM\")\n\nurls &lt;- ship_links_MAR_all$full_link\n\n#urls_test &lt;- urls[1:55]\n\n#create empty list \n\ncapacities_list &lt;- list()\n\n#fill list by looping over each link in vector\n\nfor (i in seq_along(urls)) {\n    \n    # for testing and de bugging \n    #x = urls[[57]]\n  \n    x = urls[i]\n    \n  # scrape ship info\n   table &lt;-  read_html(x) %&gt;% \n    rvest::html_nodes(\"table.infobox\") %&gt;% \n    rvest::html_table(header=F, fill = T) \n   \n    \n  # unlist to create clean table of info for each ship\n   table2 &lt;- table[[1]] %&gt;% \n    clean_names() %&gt;% \n    rename(\n      name = 1, \n      value = 2\n    ) %&gt;% \n     filter(name != \"\") %&gt;%\n     pivot_wider(names_from = name,\n                 values_from = value) \n   \n  if(\"Capacity\" %in% colnames(table2)){\n   # select columns and reshape\n   table3 &lt;- table2 %&gt;% \n       dplyr::select(`Capacity`, `Crew`) %&gt;%\n       mutate(\n       full_link = x,\n       capacity = as.character(`Capacity`),\n       crew = as.character(`Crew`)\n       ) %&gt;% \n      dplyr::select(-`Capacity`, -`Crew`)\n   \n   #print(i) # in case it breaks\n\n   capacities_list[[i]] &lt;- table3\n    \n  }else {\n    print(i) # one dropped out, url 57\n    next\n  }\n   \n}\n\n# bind all together in a dataframe\ncapacities_MAR = do.call(rbind, capacities_list) \n\n\nLast step is to clean up the ship numbers using stringr- I’ll just split it based on the space, and then grab the first element. Also need to add back in the MS FRAM since we took it out of the loop.\n\n\nCode\n#best way to get numbers out\ncapacities_MAR_clean &lt;- capacities_MAR %&gt;%\n   mutate(\n    capacity_n = as.numeric(parse_number(capacity)),\n    crew_n = as.numeric(parse_number(crew))\n  ) %&gt;% \n  mutate(\n    total_ship_pop = crew_n + capacity_n\n  ) %&gt;% \n  dplyr::select(-crew, -capacity)\n\n# rejoin with shipnames and links \n\n#FRAM df - missing info \n\nfram &lt;- data.frame(\n  shipname = \"FRAM\",\n  capacity_n = 400,\n  crew_n = 0,\n  total_ship_pop = 400\n)\n\ncruise_capacities_final &lt;- full_join(capacities_MAR_clean, ship_links_MAR_all, by = \"full_link\") %&gt;% \n  dplyr::select(shipname = name,capacity_n,crew_n,total_ship_pop) %&gt;% \n  rbind(fram)\n\n#gt(cruise_capacities_final)\nwrite_csv(cruise_capacities_final, \"outputs/cruise_capacities_final.csv\")\n\n\nVoilá! The final table with cruiseship name, passenger capacity, crew capacity, and total ship population.\n\n\nCode\ngt &lt;- cruise_capacities_final %&gt;% \n  arrange(-total_ship_pop) %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = md(\"Passenger and Crew Capacities of Cruiseships in 2019\"),\n    subtitle = md(\"Data for cruisehips present in the Mesoamerican Reef Region only\")\n  ) %&gt;% \n  cols_label(shipname = \"Ship name\",\n             capacity_n = \"Passenger Capacity\",\n             crew_n = \"Crew Capacity\",\n             total_ship_pop = \"Total Ship Population\") %&gt;% \n  tab_source_note(source_note = \"Source: List of Cruiseships Wikipedia Webpage\") %&gt;% \n  tab_options(container.overflow.y = T)\n\ngt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassenger and Crew Capacities of Cruiseships in 2019\n\n\nData for cruisehips present in the Mesoamerican Reef Region only\n\n\nShip name\nPassenger Capacity\nCrew Capacity\nTotal Ship Population\n\n\n\n\nOASIS OF THE SEAS\n5606\n2165\n7771\n\n\nALLURE OF THE SEAS\n5484\n2200\n7684\n\n\nNORWEGIAN EPIC\n4100\n1724\n5824\n\n\nNORWEGIAN BREAKAWAY\n3963\n1657\n5620\n\n\nNORWEGIAN GETAWAY\n3963\n1646\n5609\n\n\nNCL GETAWAY T6\n3963\n1646\n5609\n\n\nNORWEGIAN GETAWAY\n3963\n1646\n5609\n\n\nNCL GETAWAY T6\n3963\n1646\n5609\n\n\nDISNEY DREAM\n4000\n1458\n5458\n\n\nCARNIVAL VISTA\n3934\n1450\n5384\n\n\nLIBERTY OF THE SEAS\n3798\n1300\n5098\n\n\nADVENTURE OF THE SEA\n3807\n1185\n4992\n\n\nNAVIGATOR OF THE SEA\n3386\n1200\n4586\n\n\nEXPLORER OF THE SEAS\n3286\n1180\n4466\n\n\nCELEBRITYSILHOUETTE\n2886\n1500\n4386\n\n\nCARIBBEAN PRINCESS\n3142\n1200\n4342\n\n\nCELEBRITY REFLECTION\n3046\n1271\n4317\n\n\nMARINER OF THE SEAS\n3114\n1185\n4299\n\n\nCARNIVAL VALOR\n2974\n1180\n4154\n\n\nCARNIVAL LIBERTY\n2974\n1160\n4134\n\n\nCARNIVAL CONQUEST\n2980\n1150\n4130\n\n\nCARNIVAL FREEDOM\n2980\n1150\n4130\n\n\nCARNIVAL GLORY\n2980\n1150\n4130\n\n\nCARNIVAL SUNSHINE\n2642\n1150\n3792\n\n\nMAJESTY OF THE SEAS\n2767\n833\n3600\n\n\nNORWEGIAN PEARL\n2394\n1099\n3493\n\n\nNORWEGIAN JADE\n2402\n1037\n3439\n\n\nMSC ARMONIA\n2679\n721\n3400\n\n\nNORWEGIAN STAR\n2348\n1031\n3379\n\n\nNORWEGIAN DAWN\n2340\n1032\n3372\n\n\nDISNEY WONDER\n2400\n945\n3345\n\n\nCELEBRITY INFINITY\n2170\n999\n3169\n\n\nISLAND PRINCESS\n2214\n900\n3114\n\n\nCARNIVAL MIRACLE\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE TB1\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE TB5\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE TB1\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE TB5\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE TB1\n2124\n930\n3054\n\n\nCARNIVAL MIRACLE TB5\n2124\n930\n3054\n\n\nEURODAM\n2104\n929\n3033\n\n\nCARIBBEANPRINCE TB22\n2104\n929\n3033\n\n\nEURODAM\n2104\n929\n3033\n\n\nCARIBBEANPRINCE TB22\n2104\n929\n3033\n\n\nBRILLIANCE OFTHESEAS\n2140\n848\n2988\n\n\nNORWEGIAN SUN\n1976\n906\n2882\n\n\nMSC OPERA\n2150\n728\n2878\n\n\nOOSTERDAM\n1964\n812\n2776\n\n\nRHAPSODY OF THE SEAS\n1998\n765\n2763\n\n\nZUIDERDAM\n1916\n842\n2758\n\n\nAURORA\n1878\n850\n2728\n\n\nDISNEY MAGIC\n1750\n945\n2695\n\n\nNORWEGIAN SKY\n1928\n766\n2694\n\n\nORIANA\n1822\n794\n2616\n\n\nMARELLA DISCOVERY 2\n1836\n771\n2607\n\n\nMARELLA DISCOVERY 2\n1836\n771\n2607\n\n\nMARELLA DISCOVERY 2\n1836\n771\n2607\n\n\nMARELLA DISCOVERY 2\n1836\n771\n2607\n\n\nM/S ROTTERDAM\n1404\n600\n2004\n\n\nVEENDAM\n1350\n568\n1918\n\n\nENCHANTMENT OTS\n1350\n568\n1918\n\n\nVEENDAM\n1350\n568\n1918\n\n\nENCHANTMENT OTS\n1350\n568\n1918\n\n\nNIEUW AMSTERDAM\n1350\n520\n1870\n\n\nVIKING SUN\n740\n460\n1200\n\n\nCRYSTAL SYMPHONY\n650\n495\n1145\n\n\nAZAMARA JOURNEY\n694\n407\n1101\n\n\nREGATTA\n684\n386\n1070\n\n\nCLUB MED 2\n386\n214\n600\n\n\nSILVER WIND\n294\n208\n502\n\n\nFRAM\n400\n0\n400\n\n\nCELEBRITY EQUINOX\n2850\nNA\nNA\n\n\nINDEPENDENCE OF SEAS\nNA\nNA\nNA\n\n\n\nSource: List of Cruiseships Wikipedia Webpage"
  },
  {
    "objectID": "projects/MAR-paper/index.html",
    "href": "projects/MAR-paper/index.html",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "",
    "text": "This project was assigned to me while working as a Data Analyst at the National Center for Ecological Analysis and Synthesis (NCEAS), the first job I held after finishing my master’s degree at the University of California, Santa Barbara. I worked on the analysis from 2020 - 2021, and the paper was published in Ocean and Coastal Management in 2022.\n\n\n\n\n\nMap of study area - created using tmap\n\n\n\nLand-based nitrogen pollution is a major threat to coastal ecosystems, especially in tropical regions home to high biodiversity habitats such as coral reefs and seagrass beds. The sustained addition of excess nutrients (in the form of nitrates) to these ecosystems, which are adapted to low-nutrient environments, disrupts ecosystem function and the ability to provide services that support livelihoods and benefit human well-being. Nitrogen (N) primarily originates from agricultural crop production, livestock waste, and human sewage, as well as excretion from seabird and feral ungulates for some small atolls and cayes.\nDetermining the most effective mitigation strategies to reduce N pollution in a given location begins with identifying and quantifying input from the source. For this project, we modeled four major sources of N pollution – crop production, livestock production, wastewater generated from permanent residents and wastewater generated from seasonal populations, ie tourists – at the regional scale, measuring inputs and impacts from 430 watersheds that drain into the Mesoamerican Reef (MAR) region"
  },
  {
    "objectID": "projects/MAR-paper/index.html#project-overview",
    "href": "projects/MAR-paper/index.html#project-overview",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "",
    "text": "This project was assigned to me while working as a Data Analyst at the National Center for Ecological Analysis and Synthesis (NCEAS), the first job I held after finishing my master’s degree at the University of California, Santa Barbara. I worked on the analysis from 2020 - 2021, and the paper was published in Ocean and Coastal Management in 2022.\n\n\n\n\n\nMap of study area - created using tmap\n\n\n\nLand-based nitrogen pollution is a major threat to coastal ecosystems, especially in tropical regions home to high biodiversity habitats such as coral reefs and seagrass beds. The sustained addition of excess nutrients (in the form of nitrates) to these ecosystems, which are adapted to low-nutrient environments, disrupts ecosystem function and the ability to provide services that support livelihoods and benefit human well-being. Nitrogen (N) primarily originates from agricultural crop production, livestock waste, and human sewage, as well as excretion from seabird and feral ungulates for some small atolls and cayes.\nDetermining the most effective mitigation strategies to reduce N pollution in a given location begins with identifying and quantifying input from the source. For this project, we modeled four major sources of N pollution – crop production, livestock production, wastewater generated from permanent residents and wastewater generated from seasonal populations, ie tourists – at the regional scale, measuring inputs and impacts from 430 watersheds that drain into the Mesoamerican Reef (MAR) region"
  },
  {
    "objectID": "projects/MAR-paper/index.html#tools-and-libraries",
    "href": "projects/MAR-paper/index.html#tools-and-libraries",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "Tools and Libraries",
    "text": "Tools and Libraries\n\nAll original analysis was done in R using tidyverse, sf, raster (this was pre-terra days), purr, foreach\nGraphs were created using ggplot2 and maps were created using tmap\nI also updated and ran command line scripts written using Python and GRASS , developed and shared with me by other researchers\nMajor shout out to Coolors for helping me pick the perfect color palette"
  },
  {
    "objectID": "projects/MAR-paper/index.html#methodology",
    "href": "projects/MAR-paper/index.html#methodology",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "Methodology",
    "text": "Methodology\n\nEstimating N from Wastewater: Sourced publicly available data, as well as data from partner organizations in the Mesoamerican reef region, on hotel and other tourist accommodation locations, watershed boundaries, pourpoints, population densities, and access to wastewater treatment types (UNICEF) for an area spanning from Mexico to Honduras. (Fig 1)\nEstimating N from Agriculture and Livestock: I collaborated with the team behind the fantastic paper paper to acquire spatial crop and livestock production data. I replicated their methods for estimating N generated from each unit area of production, which was based on Fertilizer input data from the FAO and type and area of production (SPAMM). See Halpern et al (2021) for more details.\nEstimating N from Cruise ships: I used AIS ship tracking data shared by Global Fishing Watch to identify location of cruise ships present in the region in a 1 year time span. This data also allowed me to estimate how many days cruiseships spent in the MAR. I used web scraping (via Wikipedia) to generate a dataset listing active cruiseships and their human capacity, and matched ship vessel numbers to the spatial data, to ID each ship that passed through my study area in space and time. I used the capacity of the ship to estimate how much wastewater effluent each might be producing, and therefore how much Nitrogen each ship might add to the wastewater system in the watershed where it was docked, by offloading its waste*\nModeling pollution impact area in the nearshore marine environment:\n\nI quantified the amount of N from the above three sources at the approximate location the N pollution was generated, and then used a raster depicting location of surface waters (including rivers, streams, wetlands) to filter out any pollution that occured further than 1 km from a water source. This is a very “rough” way of estimating watershed transport dynamics, but I consulted a few experts and confirmed it was satisfactory for our purposes.\nI summarized N pollution within 1 km of surface waters by watershed, then assigned that amount to the corresponding pour point. I used a diffusion equation developed by Tuhloske et al in to then estimate the extent of the spatial “plume” that would result in nearshore coastal waters. Note: this script was shared with me and was written using GRASS cli. I amended and customized the script to support iterative analyses, building a loop around it. This was a major learning experience.\n\nAssessing habitat impact: I used publicly available spatial data on the spatial extent of three key marine habitats in the region: coral reefs, sea grasses, and mangroves. I intersected these with my N “plume” maps to estimate what area percentage of each habitat might be impacted by N pollution (ie Nitrogen enrichment)"
  },
  {
    "objectID": "projects/MAR-paper/index.html#key-findings",
    "href": "projects/MAR-paper/index.html#key-findings",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "Key Findings",
    "text": "Key Findings\n\n90% of the modeled nitrogen pollution was attributed to 20 watersheds, of which 11 were located in Guatemala or Honduras.\n\n\n\n\nFigure 5 from my paper - created using ggplot2\n\n\n\nAt the watershed level, watersheds from the Southern portion of the MAR contribute the most N pollution in a given year, across all sources analyzed\n\n\n\n\nFigure 4 from my paper - created using tmap\n\n\n\nThe majority of nitrogen pollution originated from crop production (52.1%) and livestock production (39.7%)\nAn estimated 80% of coral reefs and 68% of seagrass beds are at risk for exposure to anthropogenic nitrogen pollution from watershed plumes.\n\n\n\n\nFigure 5 from my paper - created using tmap"
  },
  {
    "objectID": "projects/MAR-paper/index.html#skills-i-learned-honed",
    "href": "projects/MAR-paper/index.html#skills-i-learned-honed",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "Skills I learned & honed",
    "text": "Skills I learned & honed\nThis project challenged me immensely, and I credit it with pushing me from a freshly-minted master’s student in spatial planning to seasoned data wrangler and analyst within the 1.5 years I worked on it. I performed many of these analyses on my own, sometimes with help from other project teams at NCEAS and from prior publications, but in general was working through this solo. And note this was pre-AI, so I spent A LOT of time troubleshooting and Googling, but it helped me learn so much in process.\nThe most useful skills I built from this project, with some original code snippets:\n\nweb scraping\n\n\nwriting iterative scripts for automated workflows\n\n\ncommand line interface programming\n\n\ncross-cultural collaboration & scientific publishing"
  },
  {
    "objectID": "projects/MAR-paper/index.html#acknowledgements",
    "href": "projects/MAR-paper/index.html#acknowledgements",
    "title": "Quantifying nitrogen pollution to the worlds 2nd largest barrier reef",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI had the most fantastic co-authors on this project: Steve Canty from the Smithsonian, Cascade Tuhloske ( now a professor at University of Montana) and NCEAS Director Ben Halpern. I also just want to shout out the many individuals who helped with data collection, feedback, or just general moral support along the way. Thank you to Antonella Rivera, Jenny Myton, Javier Pizaña and others from Coral Reef Alliance, Melanie McField and Ana Giro at Healthy Reefs Initiative, Alejandro Lopez and others at Centinelas de Agua, Mauricio Mejia at WWF Mesoamerica and Lauretta Burke at WRI for sharing data and their knowledge.\nThis work was funded by the Summit Foundation and the National Philanthropic Fund."
  },
  {
    "objectID": "posts_alt.html#mar-sample",
    "href": "posts_alt.html#mar-sample",
    "title": "Portfolio",
    "section": "MAR sample",
    "text": "MAR sample\n\n\nHow does this look\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "Dynamic cartography using Mapbox\n\n\nExploring new tools for spatial data visualization\n\n\n\nMadeline Berger\n\n\nApr 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping for Cruiseship Information\n\n\nUsing the rvest R package to help estimate cruiseship impact on marine habitats\n\n\n\nMadeline Berger\n\n\nJun 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying nitrogen pollution to the worlds 2nd largest barrier reef\n\n\nSpatial analysis of anthropogenic sources of Nitrogen to the Mesoamerican Reef using R and open data.\n\n\n\nMadeline Berger\n\n\nJun 11, 2021\n\n\n\n\n\n\nNo matching items"
  }
]